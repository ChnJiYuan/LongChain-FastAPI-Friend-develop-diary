# 2025.12.1 Development Log (LangChain + FastAPI Chat Companion)

## Today's Goals
- Clarify core API contracts (text chat, memory, image understanding/generation), determine minimal runnable FastAPI skeleton.
- Plan LangChain pipeline: conversation chain, RAG pipeline, tool-calling chain abstraction and data flow.
- Evaluate GPU resource integration (local LLM + SD models), define configuration entry points.

## Work Record
- Initialized FastAPI routing draft: four sub-routes (`chat`, `memory`, `image`, `generate`) with unified prefix and response format; confirmed Pydantic models for request/response encapsulation.
- Designed layered architecture: `chains/` directory organized by responsibility (conversation, rag, vision, agent, multimodal); defined input/output signatures for easy model/service replacement.
- Config entry draft: `config.py` reserves GPU detection and model path configuration (pointing to local RTX 5070 Ti, SD WebUI model directory, and SDXL Refiner weights); plan to load via environment variables/local config files.
- Logging & monitoring: decided on structured logging, plan to encapsulate loguru/standard logging in `utils/logging.py` for future request trace ID integration.
- Data & storage: RAG uses Chroma vector DB; long-term memory uses JSON (`db/user_profile.json`) for prototype, migrate to PostgreSQL later.
- Test strategy: prepare API smoke tests and chain unit tests in `tests/` using pytest + httpx.

## Issues & Solutions
- Issue: Docker daemon not running; containers unavailable short-term.
  Solution: Run FastAPI locally for now; defer Dockerfile/compose until requirements stabilize.
- Issue: Model weights are large; initial load time unpredictable.
  Solution: Implement lazy loading at startup; expose load status via health check endpoint.

## Design Insights
- Design "privacy level" field upfront: tag each message/file as public/internal/sensitive/strictly_private; only share public or consented levels with cloud.
- Add "master switch" for cloud requests: ALLOW_CLOUD_LLM=True/False for instant "local-only mode" fallback.
- Separate retrieval logic: local data handling → local; response generation → optional cloud or local.
- Sanitize logs: avoid logging full private content; use hash, ID, or truncated values for debugging.


# 2025.12.5 开发日志（LangChain + FastAPI 聊天伙伴）

## 今日目标
- 梳理核心 API（文本聊天、记忆、图像理解/生成）接口约定，确定最小可跑的 FastAPI 骨架。
- 规划 LangChain 链路：对话链、RAG 管线、工具调用链的抽象与数据流。
- 评估 GPU 资源接入方式（本地 LLM + SD 模型），明确配置入口。

## 工作记录
- 初始化 FastAPI 路由草稿：`chat`, `memory`, `image`, `generate` 四个子路由，统一前缀和响应格式；确认采用 pydantic 模型封装请求/响应。
- 设计链路分层：`chains/` 目录按职责拆分 conversation、rag、vision、agent、multimodal；定义它们的输入输出签名，便于后续替换模型或服务。
- 配置入口草案：`config.py` 预留 GPU 检测与模型路径配置（指向本机 RTX 5070 Ti、SD WebUI 模型目录与 SDXL Refiner 权重），计划通过环境变量/本地配置文件加载。
- 日志与监控：决定使用结构化日志，计划在 `utils/logging.py` 中封装 loguru/标准 logging，便于后续接入请求链路 ID。
- 数据与存储：RAG 方案初步确定使用 Chroma 向量库；长记忆使用 JSON（`db/user_profile.json`）做原型，后续再切 PostgreSQL。
- 测试策略：准备在 `tests/` 下添加 API smoke 测试与链路单测，用 pytest + httpx。

## 问题 & 解决
- 问题：Docker daemon 当前未启动，短期内无法用容器化跑依赖服务。  
  解决：本地直跑 FastAPI，暂不依赖容器；待需求稳定后再补 Dockerfile/compose。
- 问题：模型权重体积大，首次加载时间不可控。  
  解决：计划在启动时做懒加载，并通过健康检查端点暴露加载状态。

## 设计心得
- 提前设计好“隐私等级”字段

比如对每条消息/文件打标签：public / internal / sensitive / strictly_private

只有 public 或你同意的级别才允许发给云端

- 对云端请求加一个“总开关”

例如配置项 ALLOW_CLOUD_LLM=True/False

- 可以随时一键切成“全本地模式”

日志和调试注意脱敏

- 不要在 log 里记录完整的隐私内容

调试时可以只记录 hash、id 或前几个字

- 推理和 RAG 可以拆开

检索和隐私数据处理 → 全在本地完成

真正“写回答” → 可选云端 or 本地



